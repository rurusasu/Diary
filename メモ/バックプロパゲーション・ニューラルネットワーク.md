# バックプロパゲーション・ニューラルネットワーク
## 構造
バックプロパゲーション・ニューラルネットワークのアーキテクチャは以下のような構造を持つ

1. 処理ユニットがフルに相互結合した層または並びからなる階層的構造をしている. 
2. そのアーキテクチャは, 1から順に大きくなるよう番号付けられた $K$ 個の層からなる. 
3. Kolmogorovの定理より, 常に $K \geq 3$ である.
4. 第1層は, 入力ベクトル ${\bf x}$の個々の成分 $x_i$ を単に受け取り, それらを何も変えずに第2層のユニットの全てに分配する $n$ 個のファインアウト処理要素からなる.
5. 第2層から第 $K-1$ 層までは**隠れ**(hidden)層と呼ばれる.
6. 各層の各ユニットは, 下の層の各ユニットのから出力信号を受け取る. これと同じことが最後の並びまで, ネットワークの全ての層で行われる.
7. 隠れ層の各ユニットは, 上の層の各ユニットから "誤差フィードバック"結合を受ける. 
8. ネットワークの最後の ($K$番目) の並びは $m$ 個のユニットからなり, 正しい出力ベクトル ${\bf y}$ についてのネットワークの推定 ${\bf y}'$ を生ずる.  

## 実現したい処理操作
$n$ 次元ユークリッド空間のコンパクトな部分集合 $A$ から $m$ 次元ユークリッド空間の有界部分集合 $f [A]$ への有界写像, または関数 $f : A \subset {\bf R}^n \rightarrow {\bf R}^m$ を, 写像の事例 $({\bf x}_1, {\bf y}_1), ({\bf x}_2, {\bf y}_2), \cdots, ({\bf x}_k, {\bf y}_k), \cdots,$ における訓練に基づいて近似することである. ただし, ここで ${\bf y}_k = f({\bf x}_k)$ とする.

いつものように, 写像 $f$ のそのような事例は, 固定された確率密度関数 $\rho({\bf x})$ に従い, $A$ からランダムに選ばれた ${\bf x}_k$ ベクトルにより, 生成されると仮定する. 訓練後のネットワーク使用動作時においても, 必然的に, 入力ベクトル ${\bf x}$は, $\rho({\bf x})$に従いランダムに選ばれるものと仮定する. 

## 実際の処理操作
訓練時のネットワーク動作は, ネットワークを通過する2つの"スウィープ(掃引)"からなっている. 

### 前向きパス(forward pass)
前向きパスは, ベクトル${\bf x}_k$ がネットワークの第1層の中に入れられることでスタートする. 第1層の処理要素は ${\bf x}_k$ のすべての成分をネットワークの第2層のすべてのユニットに伝える. 第2層のユニットの出力は, 次に, 第3層の全てのユニットに伝えられる. 以後同様に進み, 最後に **$m$ 個の出力ユニット** (最上部の第 $K$ 並びのユニット) が, ベクトル ${\bf y}'_k$ (希望出力 ${\bf y}_k$ のネットワークによる推定) の成分を出すまで続く. 

### 後向きパス(backward pass)
推定 ${\bf y}'_k$ の出力後, ネットワーク通る後向きのスウィープをスタートさせる. まず, 出力ユニットの各々に正しい出力ベクトル ${\bf y}_k$ の成分が供給される. 出力ユニットはそれぞれ $\Delta_{Ki}$ を計算し, その値をもとに $$ 