---
marp: true
---

<!--
theme: default
size: 4:3
page_number: true
paginate: true
header: "2020年10月22日"
style: |

  section { font-size: 20px;}

  header {
    width: 100%;
    font-size: 20px;
    color: black;
    padding: 1px;
    top: 50px;
  }

  footer {
    width: 100%;
    font-size: 20px;
    color: black;
    text-align: right;
    padding: 15px;
  }

  h1 {
    font-size: 40;
    color: navy;
  }

  h2 {
    font-size: 35;
    color: navy;
  }

  h3 {
    font-size: 30;
    color: navy;
  }

  pre, code{
    font-size: 18px;
  }
-->

---

# Harris コーナー検出

初期の Computer ision におけるコーナー検出の試みとして **Chris Harris** と **Mike Stephens** が **A Combined Corner and Edge Detector** という論文で発表した **Harris コーナー** と呼ばれる方法がある。

## Harris コーナー

基本的には全方向に対して画素位置 $(u, v)$ の移動量に対する画素値の違いを見つける。

$$
E(u, v) = \sum_{x, y} w(x, y) \left[ I(x+u, y+v) - I(x, y) \right]^2
$$

ここで、$w(x, y)$ は窓関数で矩形窓か画素に対して重み付けをするガウシアン窓を使用する。

コーナー検出のためにこの関数

---

# Transformer の論文

## 自然言語処理（NLP）

### Transformer

元論文:[Attenstion is All You Need](https://arxiv.org/pdf/1706.03762.pdf)

* 翻訳タスクにおいて、$Seq2seq$ (RNN ベース Encoder-Decoder モデル) よりも早くて精度が優れている。
* RNN も CNN も使わずに **Attentionのみを使用** した Encoder-Decoder モデルで計算量も精度も改善。しかも並列計算可能。
* アーキテクチャのポイントは以下の3つ。
  * Encoder-Decoder モデル
  * Attention
  * 全結合層
* **NLPの最近のSoTA (State of The Art) たち(BERT, XLNet, GPT-2)などのベースとなるモデル**だから理解必須

---

