---
marp: true
---

<!--
theme: default
size: 4:3
page_number: true
paginate: true
header: "2020年10月22日"
style: |

  section { font-size: 20px;}

  header {
    width: 100%;
    font-size: 20px;
    color: black;
    padding: 1px;
    top: 50px;
  }

  footer {
    width: 100%;
    font-size: 20px;
    color: black;
    text-align: right;
    padding: 15px;
  }

  h1 {
    font-size: 40;
    color: navy;
  }

  h2 {
    font-size: 35;
    color: navy;
  }

  h3 {
    font-size: 30;
    color: navy;
  }

  pre, code{
    font-size: 18px;
  }
-->


# Transformer の論文

## 自然言語処理（NLP）

### Transformer

元論文:[Attenstion is All You Need](https://arxiv.org/pdf/1706.03762.pdf)

* 翻訳タスクにおいて、$Seq2seq$ (RNN ベース Encoder-Decoder モデル) よりも早くて精度が優れている。
* RNN も CNN も使わずに **Attentionのみを使用** した Encoder-Decoder モデルで計算量も精度も改善。しかも並列計算可能。
* アーキテクチャのポイントは以下の3つ。
  * Encoder-Decoder モデル
  * Attention
  * 全結合層
* **NLPの最近のSoTA (State of The Art) たち(BERT, XLNet, GPT-2)などのベースとなるモデル**だから理解必須

---

以下の図は、[サイト](https://qiita.com/omiita/items/07e69aef6c156d23c538)に掲載されていた「スペイン語-英語の翻訳タスクで cats を予測するTransformer」

<div align="center">
<img src="https://raw.githubusercontent.com/rurusasu/Diary/master/%E7%94%BB%E5%83%8F/1022/%E3%82%B9%E3%83%9A%E3%82%A4%E3%83%B3%E8%AA%9E-%E8%8B%B1%E8%AA%9E%E3%81%AE%E7%BF%BB%E8%A8%B3%E3%82%BF%E3%82%B9%E3%82%AF%E3%81%A7cats%E3%82%92%E4%BA%88%E6%B8%AC%E3%81%99%E3%82%8BTransformer.webp", width="80%">
</div>

---

### 要約

翻訳などの入力文章を別の文章で出力するというモデル ( = Sequence Transduciton Model) は [Attention](https://www.slideshare.net/yutakikuchi927/deep-learning-nlp-attention) を用いた Encoder-Decoder 形式の RNN や CNNが主流だった。

Transofmerの特徴は以下

* 再帰も畳み込みも一切使わない。
* 翻訳で今までのアンサンブルモデルも含めたSoTAを超えるBLEU スコア (28.4) をたたき出した。
* 並列化がかなりしやすく訓練時間が圧倒的に削減できる。
* それでいて **Transformerは他のタスクにも汎用性が高い**。

---

### 導入

RNN と Encoder-Decoder がこれまでの NLP 界を牽引して来たが、逐次的に単語を処理するがゆえに**訓練時に並列処理できないという大きな欠点があった**。

また、長文に関しては Attention が使われていたが、その **Attention はぼほ RNN と一緒に使われていた**。